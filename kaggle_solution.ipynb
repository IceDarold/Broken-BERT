{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fixing Broken BERT Embeddings (Fixed Padding)\n",
                "\n",
                "This notebook focuses on restoring a BERT model whose word embeddings have been corrupted. \n",
                "We use the following strategy:\n",
                "1. **Identify and Initialize**: Identify zeroed-out embeddings and initialize them with the mean of intact embeddings.\n",
                "2. **Freeze & Tune**: Freeze all transformer layers and refine only the `word_embeddings` matrix.\n",
                "3. **Pseudo-labeling**: Use the validation-tuned model to label the test data and perform a second round of fine-tuning on the combined dataset.\n",
                "\n",
                "**Restrictions**:\n",
                "- No other transformer models.\n",
                "- No additional data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import hashlib\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, pipeline\n",
                "from torch.optim import AdamW\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "import tqdm\n",
                "from sklearn.metrics import f1_score, classification_report\n",
                "\n",
                "np.random.seed(21)\n",
                "torch.manual_seed(21)\n",
                "\n",
                "VAL_PATH = \"/kaggle/input/cyprus-ai-camp-broken-bert/val_dataset.csv\"\n",
                "TEST_PATH = \"/kaggle/input/cyprus-ai-camp-broken-bert/test.csv\"\n",
                "MODEL_NAME = \"Ilseyar-kfu/broken_bert\"\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Data and Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "df_val = pd.read_csv(VAL_PATH)\n",
                "df_test = pd.read_csv(TEST_PATH)\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
                "model.to(DEVICE)\n",
                "\n",
                "print(\"Model and Tokenizer loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Embedding Analysis and Initialization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "with torch.no_grad():\n",
                "    weights = model.bert.embeddings.word_embeddings.weight.data\n",
                "    is_zero = (weights.pow(2).sum(dim=1) == 0)\n",
                "    non_zero_indices = torch.where(~is_zero)[0]\n",
                "    \n",
                "    nz_mean = weights[non_zero_indices].mean(dim=0)\n",
                "    nz_std = weights[non_zero_indices].std()\n",
                "    \n",
                "    print(f\"Total tokens: {len(weights)}\")\n",
                "    print(f\"Corrupted tokens: {is_zero.sum().item()}\")\n",
                "    \n",
                "    # Initialize corrupted ones with mean + light noise\n",
                "    weights[is_zero] = nz_mean + torch.randn_like(weights[is_zero]) * nz_std * 0.1\n",
                "\n",
                "# Freeze everything but the embeddings\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = False\n",
                "model.bert.embeddings.word_embeddings.weight.requires_grad = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "class SentimentDataset(Dataset):\n",
                "    def __init__(self, encodings, labels=None):\n",
                "        self.encodings = encodings\n",
                "        self.labels = labels\n",
                "    def __getitem__(self, idx):\n",
                "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
                "        if self.labels is not None:\n",
                "            item['labels'] = torch.tensor(self.labels[idx])\n",
                "        return item\n",
                "    def __len__(self):\n",
                "        # Check length of any key\n",
                "        return len(self.encodings['input_ids'])\n",
                "\n",
                "# Predefined mapping from baseline\n",
                "LABEL_MAP = {'neutral': 0, 'positive': 1, 'negative': 2}\n",
                "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
                "\n",
                "def train_model(model, loader, epochs, lr, warmup_steps=0):\n",
                "    optimizer = AdamW(model.bert.embeddings.word_embeddings.parameters(), lr=lr)\n",
                "    total_steps = len(loader) * epochs\n",
                "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
                "    \n",
                "    model.train()\n",
                "    for epoch in range(epochs):\n",
                "        total_loss = 0\n",
                "        for batch in tqdm.tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
                "            optimizer.zero_grad()\n",
                "            input_ids = batch['input_ids'].to(DEVICE)\n",
                "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
                "            labels = batch['labels'].to(DEVICE)\n",
                "            \n",
                "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "            loss = outputs.loss\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            scheduler.step()\n",
                "            total_loss += loss.item()\n",
                "        print(f\"Average Loss: {total_loss / len(loader):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Phase 1: Fine-tuning on Validation Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "MAX_LEN = 128\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "val_texts = df_val[\"text\"].tolist()\n",
                "val_labels = df_val[\"labels\"].map(LABEL_MAP).tolist()\n",
                "\n",
                "# CRITICAL: Use padding='max_length' to ensure consistent sizes when combining datasets\n",
                "val_encodings = tokenizer(val_texts, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
                "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "\n",
                "print(\"Starting Phase 1 Training...\")\n",
                "train_model(model, val_loader, epochs=15, lr=5e-4, warmup_steps=100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Phase 2: Pseudo-labeling and Combined Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model.eval()\n",
                "test_texts = df_test[\"text\"].tolist()\n",
                "# CRITICAL: Use padding='max_length' here too\n",
                "test_encodings = tokenizer(test_texts, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
                "test_dataset = SentimentDataset(test_encodings)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "pseudo_labels = []\n",
                "with torch.no_grad():\n",
                "    for batch in tqdm.tqdm(test_loader, desc=\"Pseudo-labeling\"):\n",
                "        input_ids = batch['input_ids'].to(DEVICE)\n",
                "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
                "        outputs = model(input_ids, attention_mask=attention_mask)\n",
                "        pseudo_labels.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
                "\n",
                "# Combine datasets - since lengths are now fixed (MAX_LEN), this won't crash in DataLoader\n",
                "combined_encodings = {k: val_encodings[k] + test_encodings[k] for k in val_encodings.keys()}\n",
                "combined_labels = val_labels + pseudo_labels\n",
                "combined_dataset = SentimentDataset(combined_encodings, combined_labels)\n",
                "combined_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "\n",
                "print(\"Starting Phase 2 Combined Training...\")\n",
                "# Use lower learning rate for combined tuning\n",
                "train_model(model, combined_loader, epochs=5, lr=1e-4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Final Evaluation on Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model.eval()\n",
                "val_preds = []\n",
                "eval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "with torch.no_grad():\n",
                "    for batch in tqdm.tqdm(eval_loader, desc=\"Evaluating\"):\n",
                "        input_ids = batch['input_ids'].to(DEVICE)\n",
                "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
                "        outputs = model(input_ids, attention_mask=attention_mask)\n",
                "        val_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
                "\n",
                "val_predict_labels = [INV_LABEL_MAP[p] for p in val_preds]\n",
                "print(\"F1 Score (Macro):\", f1_score(df_val[\"labels\"], val_predict_labels, average='macro'))\n",
                "print(classification_report(df_val[\"labels\"], val_predict_labels))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model.eval()\n",
                "final_preds = []\n",
                "with torch.no_grad():\n",
                "    for batch in tqdm.tqdm(test_loader, desc=\"Final Prediction\"):\n",
                "        input_ids = batch['input_ids'].to(DEVICE)\n",
                "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
                "        outputs = model(input_ids, attention_mask=attention_mask)\n",
                "        final_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
                "\n",
                "test_ans = [INV_LABEL_MAP[p] for p in final_preds]\n",
                "submission = pd.DataFrame({\"labels\": test_ans, \"id\": df_test[\"id\"]})\n",
                "\n",
                "hsh = hashlib.sha256(submission.to_csv(index=False).encode('utf-8')).hexdigest()[:8]\n",
                "submit_path = f\"submission_{hsh}.csv\"\n",
                "submission.to_csv(submit_path, index=False)\n",
                "submission.to_csv(\"submission.csv\", index=False)\n",
                "\n",
                "print(f\"Submission created: {submit_path}\")\n",
                "print(submission.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}