{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fixing Broken BERT Embeddings (ROBUST VERSION)\n",
                "\n",
                "**IMPORTANT**: To avoid variable collisions, please **Restart Kernel & Run All** after copying this code.\n",
                "\n",
                "Strategy:\n",
                "1. **Initialize**: Fill zeros in `word_embeddings` with mean of healthy vectors.\n",
                "2. **Phase 1**: Tune ONLY embeddings on `val_dataset.csv`.\n",
                "3. **Phase 2**: Pseudo-label `test.csv`, then tune on BOTH datasets using fixed padding.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import hashlib\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, pipeline\n",
                "from torch.optim import AdamW\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "import tqdm\n",
                "from sklearn.metrics import f1_score, classification_report\n",
                "\n",
                "np.random.seed(21)\n",
                "torch.manual_seed(21)\n",
                "\n",
                "VAL_PATH = \"/kaggle/input/cyprus-ai-camp-broken-bert/val_dataset.csv\"\n",
                "TEST_PATH = \"/kaggle/input/cyprus-ai-camp-broken-bert/test.csv\"\n",
                "MODEL_NAME = \"Ilseyar-kfu/broken_bert\"\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {DEVICE}\")\n",
                "\n",
                "MAX_LEN = 128\n",
                "BATCH_SIZE = 32\n",
                "LABEL_MAP = {'neutral': 0, 'positive': 1, 'negative': 2}\n",
                "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "class SentimentDataset(Dataset):\n",
                "    def __init__(self, encodings, labels=None):\n",
                "        self.encodings = {k: torch.tensor(v) for k, v in encodings.items()}\n",
                "        self.labels = torch.tensor(labels) if labels is not None else None\n",
                "    def __getitem__(self, idx):\n",
                "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
                "        if self.labels is not None:\n",
                "            item['labels'] = self.labels[idx]\n",
                "        return item\n",
                "    def __len__(self):\n",
                "        return len(self.encodings['input_ids'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Data and Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "df_val = pd.read_csv(VAL_PATH)\n",
                "df_test = pd.read_csv(TEST_PATH)\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
                "model.to(DEVICE)\n",
                "\n",
                "with torch.no_grad():\n",
                "    weights = model.bert.embeddings.word_embeddings.weight.data\n",
                "    is_zero = (weights.pow(2).sum(dim=1) == 0)\n",
                "    non_zero_indices = torch.where(~is_zero)[0]\n",
                "    nz_mean = weights[non_zero_indices].mean(dim=0)\n",
                "    nz_std = weights[non_zero_indices].std()\n",
                "    weights[is_zero] = nz_mean + torch.randn_like(weights[is_zero]) * nz_std * 0.1\n",
                "\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = False\n",
                "model.bert.embeddings.word_embeddings.weight.requires_grad = True\n",
                "\n",
                "print(\"Model prepared and embeddings initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Phase 1: Training on Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "val_texts = df_val[\"text\"].tolist()\n",
                "val_labels = df_val[\"labels\"].map(LABEL_MAP).tolist()\n",
                "\n",
                "val_encodings = tokenizer(val_texts, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
                "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "\n",
                "optimizer = AdamW(model.bert.embeddings.word_embeddings.parameters(), lr=5e-4)\n",
                "epochs = 15\n",
                "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(val_loader)*epochs)\n",
                "\n",
                "model.train()\n",
                "for epoch in range(epochs):\n",
                "    for batch in tqdm.tqdm(val_loader, desc=f\"P1 Epoch {epoch+1}\"):\n",
                "        optimizer.zero_grad()\n",
                "        input_ids = batch['input_ids'].to(DEVICE)\n",
                "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
                "        labels = batch['labels'].to(DEVICE)\n",
                "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
                "        outputs.loss.backward()\n",
                "        optimizer.step()\n",
                "        scheduler.step()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Phase 2: Pseudo-labeling and Combined Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model.eval()\n",
                "test_texts = df_test[\"text\"].tolist()\n",
                "test_encodings = tokenizer(test_texts, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
                "test_dataset = SentimentDataset(test_encodings)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "pseudo_labels = []\n",
                "with torch.no_grad():\n",
                "    for batch in tqdm.tqdm(test_loader, desc=\"Pseudo-labeling\"):\n",
                "        outputs = model(batch['input_ids'].to(DEVICE), attention_mask=batch['attention_mask'].to(DEVICE))\n",
                "        pseudo_labels.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
                "\n",
                "# RE-TOKENIZE EVERYTHING TO BE 100% SURE OF DIMENSIONS\n",
                "print(\"Combining and re-tokenizing...\")\n",
                "all_texts = val_texts + test_texts\n",
                "all_labels = val_labels + pseudo_labels\n",
                "\n",
                "combined_encodings = tokenizer(all_texts, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
                "combined_dataset = SentimentDataset(combined_encodings, all_labels)\n",
                "combined_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "\n",
                "optimizer = AdamW(model.bert.embeddings.word_embeddings.parameters(), lr=1e-4)\n",
                "for epoch in range(5):\n",
                "    model.train()\n",
                "    for batch in tqdm.tqdm(combined_loader, desc=f\"P2 Epoch {epoch+1}\"):\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(batch['input_ids'].to(DEVICE), attention_mask=batch['attention_mask'].to(DEVICE), labels=batch['labels'].to(DEVICE))\n",
                "        outputs.loss.backward()\n",
                "        optimizer.step()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Final Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model.eval()\n",
                "final_preds = []\n",
                "with torch.no_grad():\n",
                "    for batch in tqdm.tqdm(test_loader, desc=\"Final Pred\"):\n",
                "        outputs = model(batch['input_ids'].to(DEVICE), attention_mask=batch['attention_mask'].to(DEVICE))\n",
                "        final_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
                "\n",
                "submission = pd.DataFrame({\"labels\": [INV_LABEL_MAP[p] for p in final_preds], \"id\": df_test[\"id\"]})\n",
                "submission.to_csv(\"submission.csv\", index=False)\n",
                "print(\"Done! Output saved to submission.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}