{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RESTORING BROKEN BERT: ADVANCED VERSION\n",
                "\n",
                "Strategies implemented:\n",
                "1. **Weighted Cross-Entropy**: Penalizing neutral bias to improve recall on positive/negative.\n",
                "2. **Confident Pseudo-labeling**: Only training on high-confidence test samples (>0.85 probability).\n",
                "3. **Longer Embedding Recovery**: 30 epochs for initial tuning + 10 epochs for combined tuning.\n",
                "4. **Scheduler with Warmup**: Gradual LR increase to stabilize embedding recovery.\n",
                "\n",
                "**Note**: Restart Kernel before running."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch.nn.functional as F\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
                "from torch.optim import AdamW\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "import tqdm\n",
                "from sklearn.metrics import f1_score, classification_report\n",
                "\n",
                "np.random.seed(42)\n",
                "torch.manual_seed(42)\n",
                "\n",
                "VAL_PATH = \"/kaggle/input/cyprus-ai-camp-broken-bert/val_dataset.csv\"\n",
                "TEST_PATH = \"/kaggle/input/cyprus-ai-camp-broken-bert/test.csv\"\n",
                "MODEL_NAME = \"Ilseyar-kfu/broken_bert\"\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "MAX_LEN = 128\n",
                "BATCH_SIZE = 32\n",
                "LABEL_MAP = {'neutral': 0, 'positive': 1, 'negative': 2}\n",
                "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
                "\n",
                "print(f\"Device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "class SentimentDataset(Dataset):\n",
                "    def __init__(self, encodings, labels=None):\n",
                "        self.encodings = {k: torch.tensor(v) for k, v in encodings.items()}\n",
                "        self.labels = torch.tensor(labels) if labels is not None else None\n",
                "    def __getitem__(self, idx):\n",
                "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
                "        if self.labels is not None:\n",
                "            item['labels'] = self.labels[idx]\n",
                "        return item\n",
                "    def __len__(self):\n",
                "        return len(self.encodings['input_ids'])\n",
                "\n",
                "def train_step(model, loader, optimizer, scheduler, criterion):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    for batch in tqdm.tqdm(loader, leave=False):\n",
                "        optimizer.zero_grad()\n",
                "        input_ids = batch['input_ids'].to(DEVICE)\n",
                "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
                "        labels = batch['labels'].to(DEVICE)\n",
                "        \n",
                "        outputs = model(input_ids, attention_mask=attention_mask)\n",
                "        loss = criterion(outputs.logits, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        scheduler.step()\n",
                "        total_loss += loss.item()\n",
                "    return total_loss / len(loader)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "df_val = pd.read_csv(VAL_PATH)\n",
                "df_test = pd.read_csv(TEST_PATH)\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
                "model.to(DEVICE)\n",
                "\n",
                "# 1. Initialize Zeros\n",
                "with torch.no_grad():\n",
                "    weights = model.bert.embeddings.word_embeddings.weight.data\n",
                "    is_zero = (weights.pow(2).sum(dim=1) == 0)\n",
                "    nz_mean = weights[~is_zero].mean(dim=0)\n",
                "    nz_std = weights[~is_zero].std()\n",
                "    weights[is_zero] = nz_mean + torch.randn_like(weights[is_zero]) * nz_std * 0.1\n",
                "\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = False\n",
                "model.bert.embeddings.word_embeddings.weight.requires_grad = True\n",
                "\n",
                "print(f\"Corrupted tokens initialized: {is_zero.sum().item()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Phase 1: Heavy Tuning with Class Weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "val_texts = df_val[\"text\"].tolist()\n",
                "val_labels = df_val[\"labels\"].map(LABEL_MAP).tolist()\n",
                "\n",
                "val_encodings = tokenizer(val_texts, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
                "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "\n",
                "# Class Weights: Increase importance of positive(1) and negative(2)\n",
                "class_weights = torch.tensor([1.0, 2.5, 2.5]).to(DEVICE)\n",
                "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
                "\n",
                "optimizer = AdamW(model.bert.embeddings.word_embeddings.parameters(), lr=8e-4)\n",
                "epochs = 30\n",
                "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=200, num_training_steps=len(val_loader)*epochs)\n",
                "\n",
                "print(\"Phase 1: Recovery Loop\")\n",
                "for epoch in range(epochs):\n",
                "    loss = train_step(model, val_loader, optimizer, scheduler, criterion)\n",
                "    if (epoch + 1) % 5 == 0:\n",
                "        print(f\"Epoch {epoch+1} Loss: {loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Phase 2: High-Confidence Pseudo-Labeling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model.eval()\n",
                "test_texts = df_test[\"text\"].tolist()\n",
                "test_encodings = tokenizer(test_texts, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
                "test_dataset = SentimentDataset(test_encodings)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "probs_list = []\n",
                "preds_list = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for batch in tqdm.tqdm(test_loader, desc=\"Predicting Test\"):\n",
                "        logits = model(batch['input_ids'].to(DEVICE), attention_mask=batch['attention_mask'].to(DEVICE)).logits\n",
                "        probs = F.softmax(logits, dim=1)\n",
                "        conf, pred = torch.max(probs, dim=1)\n",
                "        probs_list.extend(conf.cpu().numpy())\n",
                "        preds_list.extend(pred.cpu().numpy())\n",
                "\n",
                "CONFIDENCE_THRESHOLD = 0.85\n",
                "test_df_pseudo = pd.DataFrame({'text': test_texts, 'label': preds_list, 'prob': probs_list})\n",
                "confident_test = test_df_pseudo[test_df_pseudo['prob'] >= CONFIDENCE_THRESHOLD]\n",
                "\n",
                "print(f\"Using {len(confident_test)} / {len(df_test)} samples from test for Phase 2\")\n",
                "\n",
                "# Combine data\n",
                "p2_texts = val_texts + confident_test['text'].tolist()\n",
                "p2_labels = val_labels + confident_test['label'].tolist()\n",
                "\n",
                "p2_encodings = tokenizer(p2_texts, truncation=True, padding='max_length', max_length=MAX_LEN)\n",
                "p2_dataset = SentimentDataset(p2_encodings, p2_labels)\n",
                "p2_loader = DataLoader(p2_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "\n",
                "# Second stage tuning (lower LR)\n",
                "optimizer = AdamW(model.bert.embeddings.word_embeddings.parameters(), lr=1e-4)\n",
                "epochs_p2 = 10\n",
                "scheduler_p2 = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(p2_loader)*epochs_p2)\n",
                "\n",
                "print(\"Phase 2: Combined Tuning\")\n",
                "for epoch in range(epochs_p2):\n",
                "    loss = train_step(model, p2_loader, optimizer, scheduler_p2, criterion)\n",
                "    if (epoch + 1) % 2 == 0:\n",
                "        print(f\"P2 Epoch {epoch+1} Loss: {loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Final Evaluation & Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "model.eval()\n",
                "final_preds = []\n",
                "with torch.no_grad():\n",
                "    for batch in tqdm.tqdm(test_loader, desc=\"Final Pred\"):\n",
                "        logits = model(batch['input_ids'].to(DEVICE), attention_mask=batch['attention_mask'].to(DEVICE)).logits\n",
                "        final_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
                "\n",
                "submission = pd.DataFrame({\"labels\": [INV_LABEL_MAP[p] for p in final_preds], \"id\": df_test[\"id\"]})\n",
                "submission.to_csv(\"submission.csv\", index=False)\n",
                "print(\"Submission saved!\")\n",
                "\n",
                "# Validation Report\n",
                "val_preds = []\n",
                "eval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "with torch.no_grad():\n",
                "    for batch in eval_loader:\n",
                "        logits = model(batch['input_ids'].to(DEVICE), attention_mask=batch['attention_mask'].to(DEVICE)).logits\n",
                "        val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
                "\n",
                "print(f1_score(df_val[\"labels\"], [INV_LABEL_MAP[p] for p in val_preds], average='macro'))\n",
                "print(classification_report(df_val[\"labels\"], [INV_LABEL_MAP[p] for p in val_preds]))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}